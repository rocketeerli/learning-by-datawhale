第二天

过拟合、欠拟合及其解决方案；梯度消失、梯度爆炸、梯度偏移；卷积神经网络基础

### 过拟合、欠拟合

* 多项式拟合

`plt.semilogy()`: y 轴使用对数尺度，画折线图

一阶多项式函数拟合又叫线性函数拟合。

* 验证集

预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。

选择模型时，不仅依靠训练误差，还依赖于泛化误差。

通用的方法：K折交叉验证。

#### 解决过拟合

* 权值衰减

通用的方法是增加 `L2` 正则项

* 丢弃法

丢弃法不改变其输入的期望值。

使得下一层的计算不过分依赖上一层的某一具体单元。一定程度上起到正则化的效果。

### 梯度消失、梯度爆炸以及梯度偏移

* 随机初始化模型参数

如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。

**Xavier随机初始化**：前向传播和反向传播时，确保每一层输出的方差应该尽量相等。

### 卷积神经网络基础

* 互相关运算与卷积运算

卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。

**互相关运算**：卷积核与该位置处的输入子数组按元素相乘并求和。

**卷积运算**：将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。

由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。

* 感受野

元素 `x` 的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做 `x` 的感受野（receptive field）。

通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。

* 填充

对于高度（或宽度）为大小为 `2k + 1` 的核，令步幅为1，在高（或宽）两侧选择大小为 `k` 的填充，便可保持输入与输出尺寸相同。

* 多通道

一个核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个核数组，不同的核数组提取的是不同的特征。

`1 x 1` 卷积核可在不改变高宽的情况下，调整通道数。

* 卷积层与全连接层的对比

1. 全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地**具有提取局部信息的能力**。
2. 卷积层的参数量更少。**卷积核的参数量与输入图像的宽高无关**。使用卷积层可以以较少的参数数量来处理更大的图像。

* 池化层

池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着**池化层的输出通道数与输入通道数相等**。
