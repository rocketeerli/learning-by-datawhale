第七天

优化算法进阶；数据增强；模型微调

## 优化算法进阶

* Momentum

需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。

加入动量，代表前面迭代次数的梯度指数加权平均值，越接近当前迭代次数的梯度，权值越大。

动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。

* AdaGrad

AdaGrad算法，根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。

AdaGrad算法，由于一直在累加梯度的平方，最后的效果会使学习率不断衰减。

问题：**AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解**。

* RMSProp

AdaGrad 算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp 算法对 AdaGrad 算法做了修改。

1. RMSProp算法将这些梯度按元素平方做指数加权移动平均。

2. 和 AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量。

算法的状态变量是对平方项的指数加权移动平均，所以可以看作是最近多个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。

在同样的学习率下，RMSProp 算法可以比 AdaGrad 算法更快逼近最优解。

* AdaDelta

AdaDelta 算法也针对 AdaGrad 算法在迭代后期可能较难找到有用解的问题做了改进。

**AdaDelta 算法没有学习率这一超参数**。

AdaDelta 算法与 RMSProp 算法的不同之处在于，分子中的超参数的不同。

AdaDelta算法需要对每个自变量维护两个状态变量。不仅对梯度进行移动加权平均，还对自变量的变化量进行移动加权平均。

* Adam

Adam 算法使用了动量变量 m 和 RMSProp 算法中小批量随机梯度按元素平方的指数加权移动平均变量 v。

这里需要除以一个系数，从而使过去各时间步小批量随机梯度权值之和为 1。

和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。
