第六天

优化算法进阶；数据增强；模型微调

## 优化算法进阶

* Momentum

加入动量，代表前面迭代次数的梯度指数加权平均值，越接近当前迭代次数的梯度，权值越大。

动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。

* AdaGrad

AdaGrad算法，根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。

AdaGrad算法，由于一直在累加梯度的平方，最后的效果会使学习率不断衰减。

问题：**AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解**。
