第七天

优化算法进阶；数据增强；模型微调

## 优化算法进阶

* Momentum

需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。

加入动量，代表前面迭代次数的梯度指数加权平均值，越接近当前迭代次数的梯度，权值越大。

动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。

* AdaGrad

AdaGrad算法，根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。

AdaGrad算法，由于一直在累加梯度的平方，最后的效果会使学习率不断衰减。

问题：**AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解**。

* RMSProp

AdaGrad 算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp 算法对 AdaGrad 算法做了修改。

1. RMSProp算法将这些梯度按元素平方做指数加权移动平均。

2. 和 AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量。

算法的状态变量是对平方项的指数加权移动平均，所以可以看作是最近多个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。

在同样的学习率下，RMSProp 算法可以比 AdaGrad 算法更快逼近最优解。

* AdaDelta

AdaDelta 算法也针对 AdaGrad 算法在迭代后期可能较难找到有用解的问题做了改进。

**AdaDelta 算法没有学习率这一超参数**。

AdaDelta 算法与 RMSProp 算法的不同之处在于，分子中的超参数的不同。

AdaDelta算法需要对每个自变量维护两个状态变量。不仅对梯度进行移动加权平均，还对自变量的变化量进行移动加权平均。

* Adam

Adam 算法使用了动量变量 m 和 RMSProp 算法中小批量随机梯度按元素平方的指数加权移动平均变量 v。

这里需要除以一个系数，从而使过去各时间步小批量随机梯度权值之和为 1。

和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。

## 数据增强

大规模数据集是成功应用深度神经网络的前提。

图像增广（image augmentation）技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。

另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。

### 常用的数据曾广的方法

1. 翻转

上下翻转不如左右翻转通用。

可以通过 torchvision.transforms 模块创建 RandomHorizontalFlip 实例来实现一半概率的图像水平（左右）翻转。

垂直（上下）翻转是 torchvision.transforms.RandomVerticalFlip

2. 裁剪

池化层能降低卷积层对目标位置的敏感度。

除此之外，我们还可以通过对图像随机裁剪来让物体以不同的比例出现在图像的不同位置，这同样能够降低模型对目标位置的敏感性。

使用 `torchvision.transforms.RandomResizedCrop` 进行随机裁剪。

3. 变化颜色

使用 `torchvision.transforms.ColorJitter` 变化图像的亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue）等。

4. 叠加多个图像增广方法

实际应用中我们会将多个图像增广方法叠加使用。

可以通过 Compose 实例将上面定义的多个图像增广方法叠加起来，再应用到每张图像之上。

## 模型微调

迁移学习（transfer learning）：将从源数据集学到的知识迁移到目标数据集上。

微调的步骤：

1. 在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。

2. 创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。 我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。

3. 为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。

4. 在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。

当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。
