第一天

线性回归；Softmax与分类模型、多层感知机

### 线性回归

LinearRegression.py 分别介绍了从零开始实现的线性回归和利用 PyTorch 实现的简洁版

从零开始也不是完全不使用 torch，只是没有直接使用 `torch.nn` 和 `torch.optim` 来创建模型并训练


### softmax

* softmax 函数

softmax 可以将神经元的输出映射到(0, 1) 区间，且和为一，这样就可以直接使用概率的方式来解释输出。

最大的特点是，求导计算很方便。尤其是跟交叉熵损失函数结合起来，求得的梯度很简洁。

即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。

* 交叉熵损失函数

交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。

* 对多维 Tensor 按维度操作 

`X.sum(dim=0, keepdim=True)` dim为0，按照相同的列求和，并在结果中保留列特征

`y_hat.gather(1, y.view(-1, 1))` 收集输入的特定维度指定位置的数值

这里的特定维度是指 dim 参数，如这个例子里，是对列进行操作。

三维 tensor 的元素选择

	out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
	out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
	out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2

* 可调用对象

但凡是可以把一对括号()应用到某个对象身上都可称之为可调用对象

如果在类中实现了 `__call__` 方法，那么实例对象也将成为一个可调用对象


### 多层感知机

* 激活函数的必要性

全连接层只是对数据做线性变换，多个线性变换的叠加仍然是一个线性变换。

因此需要增加激活函数，为了引入非线性变换

* 常用的激活函数

ReLU: 只保留正数元素，并将负数元素清零

sigmoid: 将元素的值变换到0和1之间

tanh（双曲正切）: 可以将元素的值变换到-1和1之间

* 激活函数特点

ReLU 只能在隐藏层中使用。

由于梯度消失问题，有时要避免使用sigmoid和tanh函数。

在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。

* 多层感知机定义

多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。

* nn.CrossEntropyLoss()

pytorch 的交叉熵损失函数 `nn.CrossEntropyLoss()` 已经内置了 Softmax 不需要再对结果使用 softmax

这里需要注意一下，去年深度学习课程实验写错了，现在才知道。

**注意输入模型的维度**
