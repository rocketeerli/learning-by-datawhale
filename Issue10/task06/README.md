第六天

Transformer、凸优化；梯度下降

## Transformer

### CNN 和 RNN 比较

* CNNs: 易于并行化，却不适合捕捉变长序列内的依赖关系。

* RNNs: 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。

Transformer 模型利用 attention 机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens。

上述优势使得 Transformer 模型在性能优异的同时大大减少了训练时间。

### 自注意力（self-attention）

自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。

与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。

### 多头注意力层（Multi-head Attention Layers）

多头注意力层包含 h 个并行的自注意力层，每一个这种层被成为一个head。

对每个头来说，在进行注意力计算之前，我们会将 query、key 和 value 用三个现行层进行映射。

这个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。
