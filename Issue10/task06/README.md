第六天

Transformer、凸优化；梯度下降

## Transformer

### CNN 和 RNN 比较

* CNNs: 易于并行化，却不适合捕捉变长序列内的依赖关系。

* RNNs: 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。

Transformer 模型利用 attention 机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens。

上述优势使得 Transformer 模型在性能优异的同时大大减少了训练时间。

### 自注意力（self-attention）

自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。

与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。

### 1. 多头注意力层（Multi-head Attention Layers）

多头注意力层包含 h 个并行的自注意力层，每一个这种层被成为一个head。

对每个头来说，在进行注意力计算之前，我们会将 query、key 和 value 用三个现行层进行映射。

这个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。

### 2. 基于位置的前馈网络

Position-wise FFN由两个全连接层组成，他们作用在最后一维上。（名字很高大上，其实没啥新东西。。。）

因为序列的每个位置的状态都会被单独地更新，所以我们称他为position-wise，这等效于一个1x1的卷积。

### 3. 相加归一化层(Add and Norm)

这里 Layer Norm 与 Batch Norm 很相似，唯一的区别在于 Batch Norm 是对于 batch size 这个维度进行计算均值和方差的，而 Layer Norm 则是对最后一维进行计算。

### 4. 位置编码

与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。

位置编码是一个二维的矩阵，i对应着序列中的顺序，j对应其embedding vector内部的维度索引。

### 编码器模块（EncoderBlock）

编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。

### Transformer 编码器

整个编码器由n个刚刚定义的 Encoder Block 堆叠而成，因为残差连接的缘故，中间状态的维度始终与嵌入向量的维度d一致。

把嵌入向量乘以 根号d 以防止其值过小。

### Transformer 解码器

Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，编码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。

与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。

## 凸优化

* 优化方法：训练集损失函数值

* 深度学习目标：测试集损失函数值（泛化性）

重点理解**局部最小值、鞍点、梯度消失、凸函数、Jensen 不等式、拉格朗日乘子法和惩罚项**。
