第五天

循环神经网络进阶；机器翻译及相关技术；注意力机制与Seq2seq模型

## 循环神经网络进阶

### GRU

RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）。

门控循环神经网络：捕捉时间序列中时间步距离较长的依赖关系。

* 重置门：有助于捕捉时间序列里短期的依赖关系。
* 更新门：有助于捕捉时间序列里长期的依赖关系。

GRU 很聪明的一点就在于，我们使用了同一个门控，就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。

### LSTM

长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。

* 遗忘门: 控制上一时间步的记忆细胞。
* 输入门: 控制当前时间步的输入。
* 输出门: 控制从记忆细胞到隐藏状态。

* 记忆细胞：一种特殊的隐藏状态的信息的流动。

### 深度循环神经网络和双向循环神经网络

深度循环神经网络没得介绍了，就是单纯增加网络层数，隐藏层不止一层。

双向循环神经网络需要注意的一点是，前向和后向RNN连结的方式是： **前向的 H_t 和后向的 H_t 用concat进行连结**。

## 机器翻译及相关技术

机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。

主要特征：

1. 输出是单词序列而不是单个单词。

2. 输出序列的长度可能与源序列的长度不同。

### Encoder-Decoder

* encoder：输入到隐藏状态

* decoder：隐藏状态到输出

Encoder-Decoder并不是一个具体的模型，而是一类框架。Encoder和Decoder部分可以是任意的文字，语音，图像，视频数据，模型可以采用CNN，RNN，BiRNN、LSTM、GRU等等。

encoder-decoder模型虽然非常经典，但是局限性也非常大。最大的局限性就在于编码和解码之间的唯一联系就是一个固定长度的语义向量C。

可以应用在对话系统、生成式任务中。

### Sequence to Sequence 模型

* torch.nn.Embedding

一个简单的存储固定大小的词典的嵌入向量的查找表。默认随机赋值，也可以手动加载权重。

* torch.transpose()

pytorch中的transpose方法的作用是交换矩阵的两个维度，transpose(dim0, dim1) → Tensor

transpose 中的两个维度参数的顺序是可以交换位置的，即transpose（x, 0, 1,) 和transpose（x, 1, 0）效果是相同的。

* [:,None]

None表示该维不进行切片，而是将该维整体作为数组元素处理。

所以，[:,None] 的效果就是将二维数组按每行分割，最后形成一个三维数组。

## 注意力机制与Seq2seq模型

没有引入注意力的模型在输入句子比较短的时候问题不大。

但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失。

可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。

### Sequence to Sequence 存在的问题

1. 解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。

2. 同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。

在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。

* 超出2维矩阵的乘法

X 和 Y 是维度分别为 (b, n, m) 和 (b, m, k) 的张量，进行 b 次二维矩阵乘法后得到 Z, Z 的维度为 (b, n, k)。

### attention layer

Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。

key 和 value 的数目相同，维度不一定相同。query 和 key-value 维度和数目都不一定相同。

attention layer 得到输出与 value 的维度一致。输出数目与 query 数目一致。

### 点积注意力(The dot product)

假设 query 和 keys 有相同的维度，可以通过计算 query 和 key 转置的乘积来计算 attention score。

通常还会除去 d 的平方根，减少计算出来的 score 对维度 d 的依赖性。

#### 机器翻译里的注意力机制

在机器翻译里，因为在计算 Attention 的过程中，quqery 是模型的hidden，即隐藏层的输出；Key 和 Value 指向的是同一个东西——输入句子中每个单词对应的语义编码，即 Encoder 的 outputs。
