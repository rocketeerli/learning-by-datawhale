第五天

循环神经网络进阶；机器翻译及相关技术；注意力机制与Seq2seq模型

## 循环神经网络进阶

### GRU

RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）。

门控循环神经网络：捕捉时间序列中时间步距离较长的依赖关系。

* 重置门：有助于捕捉时间序列里短期的依赖关系。
* 更新门：有助于捕捉时间序列里长期的依赖关系。

GRU 很聪明的一点就在于，我们使用了同一个门控，就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。

### LSTM

长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。

* 遗忘门: 控制上一时间步的记忆细胞。
* 输入门: 控制当前时间步的输入。
* 输出门: 控制从记忆细胞到隐藏状态。

* 记忆细胞：一种特殊的隐藏状态的信息的流动。
