第五天

循环神经网络进阶；机器翻译及相关技术；注意力机制与Seq2seq模型

## 循环神经网络进阶

### GRU

RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）。

门控循环神经网络：捕捉时间序列中时间步距离较长的依赖关系。

* 重置门：有助于捕捉时间序列里短期的依赖关系。
* 更新门：有助于捕捉时间序列里长期的依赖关系。

GRU 很聪明的一点就在于，我们使用了同一个门控，就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。

### LSTM

长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。

* 遗忘门: 控制上一时间步的记忆细胞。
* 输入门: 控制当前时间步的输入。
* 输出门: 控制从记忆细胞到隐藏状态。

* 记忆细胞：一种特殊的隐藏状态的信息的流动。

### 深度循环神经网络和双向循环神经网络

深度循环神经网络没得介绍了，就是单纯增加网络层数，隐藏层不止一层。

双向循环神经网络需要注意的一点是，前向和后向RNN连结的方式是： **前向的 H_t 和后向的 H_t 用concat进行连结**。

## 机器翻译及相关技术

机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。

主要特征：

1. 输出是单词序列而不是单个单词。

2. 输出序列的长度可能与源序列的长度不同。
