第十天

word2vec；词嵌入进阶；文本分类

## word2vec

把词映射为实数域向量的技术也叫词嵌入。

### 词嵌入基础

之前通常是使用 one-hot 向量表示单词，虽然它们构造起来很容易，但通常并不是一个好选择。

一个主要的原因是，one-hot 词向量无法准确表达不同词之间的相似度。

* Skip-Gram 跳字模型

跳字模型假设基于某个词来生成它在文本序列周围的词。

在跳字模型中，每个词被表示成两个 d 维向量，用来计算条件概率。

假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为 m 时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率。

跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。

训练结束后，对于词典中的任一索引为 i 的词，我们均得到该词作为中心词和背景词的两组词向量 vi 和 ui 。在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。

* CBOW (continuous bag-of-words) 连续词袋模型

连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词。

因为连续词袋模型的背景词有多个，我们将这些背景词向量取平均，然后使用和跳字模型一样的方法来计算条件概率。

同跳字模型不一样的一点在于，我们一般使用连续词袋模型的背景词向量作为词的表征向量。

* Word2Vec & PTB

Word2Vec 能从语料中学到如何将离散的词映射为连续空间中的向量，并保留其语义上的相似关系。

那么为了训练 Word2Vec 模型，我们就需要一个自然语言语料库，模型将从中学习各个单词间的关系，这里我们使用经典的 PTB 语料库进行训练。

PTB (Penn Tree Bank) 是一个常用的小型语料库，它采样自《华尔街日报》的文章，包括训练集、验证集和测试集。我们将在PTB训练集上训练词嵌入模型。

* 二次采样

通常来说，在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。

因此，训练词嵌入模型时可以对词进行二次采样。 具体来说，数据集中每个被索引词 Wi 将有一定概率被丢弃。

* 负采样近似

没看懂这块。。。nlp有点难。。。
